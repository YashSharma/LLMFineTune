### Data-Driven Alignment of Large Language Models for Enhanced Safety and Effectiveness

This study investigates the crucial alignment steps of Large Language Models (LLMs) using the Llama-7b model as a case study. Focusing on safety and helpfulness enhancement, we explore the impact of human-generated versus LLM-generated datasets on model performance. Leveraging Eleuther AI benchmarks and SafeNLP’s demographic assessment, we discern performance variations across diverse groups. Our key finding underscores the piv- otal role of dataset curation, with the poorly curated SHP dataset showing a modest impact and a well-curated smaller LIMA dataset yield- ing substantial gains. Notably, well-curated LLM-generated comparison datasets demonstrate impressive safety gains in LLM after the DPO step. This highlights that SFT can help in reasoning improvement, but it is crucial to perform DPO/ RLHF on well-curated comparison data to improve the model’s safety. This research sheds light on nuanced dynamics in fine-tuning, providing valuable insights for AI alignment. 
